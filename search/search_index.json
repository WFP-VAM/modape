{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MODAPE Overview The MOD IS A ssimilation and P rocessing E ngine combines a state-of-the-art whittaker smoother, implemented as fast C-extension through Cython, with a HDF5 based processing chain optimized for MODIS data. The implementation of the Whittaker filter includes a V-curve based optimization of the smoothing parameter, which allows a pixel by pixel variation in the degree of smoothing applied, directly derived from the pixel\u2019s timeseries. In addition, MODAPE implements expectile smoothing to estimate a smoothly varying envelope of the input signal. Most users will want to use the out-of-the-box processing chain, which features pre-tuned parameters for running the smoother and command line executables, that enable the user to run the entire processing chain with minimal input from the command line. Note While there are no restrictions on which MODIS data can be downloaded, the full processing chain is (currently) only implemented for MOD/MYD 11&13 products (NDVI, EVI, LST). The chain breaks down into 4 separate steps, each executable with a dedicated command line script, that handle everything from downloading raw MODIS data, to filtering the data and exporting it as GeoTIFF for downstream analysis and visualization: MODIS processing chain in MODAPE modis_download Query and download MODIS data to local storage (requires Earthdata credentials ) modis_collect Collect raw NASA HDF files into raw HDF5 file(s) modis_smooth Run Whittaker smoother on raw HDF5 file(s) modis_window Export data from raw/smooth HDF5 file(s) as GeoTIFF Installation MODAPE builds on GDAL and expects working python bindings . Both can be a bit tricky to set up correctly, but here's a couple of tips for installing it poperly depending on the system: Ubuntu : Check out UbuntuGIS Windows : Chris Gohlke has an amazing collection of unofficial binary wheels, one of them is GDAL for various version of python Installation with python/pip Note: It's recommended to use a virtual environment from PyPI pip3 install modape from GitHub git clone https://github.com/WFP-VAM/modape.git cd modape pip3 install . or with setup.py : git clone https://github.com/WFP-VAM/modape.git cd modape python3 setup.py install Installation with Docker git clone https://github.com/WFP-VAM/modape.git cd modape docker build -t modape . # check if it's working docker run --rm -it modape modape_version Quick guide to naming convention Note Since MODAPE was developed for WFP VAM's operational needs, it heavily uses an established naming convention for different variables etc. The naming for both raw and smooth HDF5 files will be constructed from: MODIS product type tile index (if applicable) temporal interpolation code (for smooth files) MODIS collection number VAM parameter code VAM parameter codes Parameter code Reference VIM MODIS Normalized Difference Vegetation Index (NDVI) VEM MODIS Enhanced Vegetation Index (EVI) TDA MODIS Aqua Daytime Land Surface Temperature TNA MODIS Aqua Nighttime Land Surface Temperature TDT MODIS Terra Daytime Land Surface Temperature TNT MODIS Terra Nighttime Land Surface Temperature Temporal interpolation codes Code Reference TXN Native temporal resolution (no interpolation performed) TXD 10-day timestep (dekads) TXP 5-day timestep (pentads) TXC Custom interpolation (as defined by user) Examples Raw HDF5 file Example MOD13A2.h20v08.006.VIM.h5 : this is an example for 1km MODIS NDVI, collection 6, for the tile h20v08 from the Terra satellite MXD13A2.h20v08.006.VIM.h5 : if both Aqua and Terra get interleaved, the product code changes to MXD MXD13C1.006.VIM.h5 : if the product is global, the filename does not include a tile index Smooth HDF5 file The filename for a smooth HDF5 file is directly derived from the raw input, so all elements of the raw file will be adopted. The only thing changing is the added information of the temporal interpolation. Example MXD13A2.h20v08.006.txn.VIM.h5 : smooth version of the rawfile above, without temporal interpolation (so in native timestep) MXD13A2.h20v08.006.txd.VIM.h5 : same as above, just with temporal interpolation performed to 10-day (dekad) timestep Exported GeoTIFFs The naming for the exported GeoTIFFs is put together from the region code (supplied by the user - if not, the default is reg ), VAM code and the timestamp (either in julian format or dekads/pentads if applicable). Note The filenames don't contain any information about the product or tile, so GeoTIFFs from different files can end up having the same name and be easily overwritten. Caution is advised! Example regvim2020001.tif : basic example for an exported NDVI image for either a raw HDF5 file, or a smooth HDF5 file with no or custom temporal interpolation regvim2020d1.tif : If the temporal interpolation is dekad, the timestamps are converted to dekads (this behavior can be prevented with a flag in the executable)","title":"Home"},{"location":"#modape","text":"","title":"MODAPE"},{"location":"#overview","text":"The MOD IS A ssimilation and P rocessing E ngine combines a state-of-the-art whittaker smoother, implemented as fast C-extension through Cython, with a HDF5 based processing chain optimized for MODIS data. The implementation of the Whittaker filter includes a V-curve based optimization of the smoothing parameter, which allows a pixel by pixel variation in the degree of smoothing applied, directly derived from the pixel\u2019s timeseries. In addition, MODAPE implements expectile smoothing to estimate a smoothly varying envelope of the input signal. Most users will want to use the out-of-the-box processing chain, which features pre-tuned parameters for running the smoother and command line executables, that enable the user to run the entire processing chain with minimal input from the command line. Note While there are no restrictions on which MODIS data can be downloaded, the full processing chain is (currently) only implemented for MOD/MYD 11&13 products (NDVI, EVI, LST). The chain breaks down into 4 separate steps, each executable with a dedicated command line script, that handle everything from downloading raw MODIS data, to filtering the data and exporting it as GeoTIFF for downstream analysis and visualization: MODIS processing chain in MODAPE modis_download Query and download MODIS data to local storage (requires Earthdata credentials ) modis_collect Collect raw NASA HDF files into raw HDF5 file(s) modis_smooth Run Whittaker smoother on raw HDF5 file(s) modis_window Export data from raw/smooth HDF5 file(s) as GeoTIFF","title":"Overview"},{"location":"#installation","text":"MODAPE builds on GDAL and expects working python bindings . Both can be a bit tricky to set up correctly, but here's a couple of tips for installing it poperly depending on the system: Ubuntu : Check out UbuntuGIS Windows : Chris Gohlke has an amazing collection of unofficial binary wheels, one of them is GDAL for various version of python","title":"Installation"},{"location":"#installation-with-pythonpip","text":"Note: It's recommended to use a virtual environment","title":"Installation with python/pip"},{"location":"#from-pypi","text":"pip3 install modape","title":"from PyPI"},{"location":"#from-github","text":"git clone https://github.com/WFP-VAM/modape.git cd modape pip3 install . or with setup.py : git clone https://github.com/WFP-VAM/modape.git cd modape python3 setup.py install","title":"from GitHub"},{"location":"#installation-with-docker","text":"git clone https://github.com/WFP-VAM/modape.git cd modape docker build -t modape . # check if it's working docker run --rm -it modape modape_version","title":"Installation with Docker"},{"location":"#quick-guide-to-naming-convention","text":"Note Since MODAPE was developed for WFP VAM's operational needs, it heavily uses an established naming convention for different variables etc. The naming for both raw and smooth HDF5 files will be constructed from: MODIS product type tile index (if applicable) temporal interpolation code (for smooth files) MODIS collection number VAM parameter code","title":"Quick guide to naming convention"},{"location":"#vam-parameter-codes","text":"Parameter code Reference VIM MODIS Normalized Difference Vegetation Index (NDVI) VEM MODIS Enhanced Vegetation Index (EVI) TDA MODIS Aqua Daytime Land Surface Temperature TNA MODIS Aqua Nighttime Land Surface Temperature TDT MODIS Terra Daytime Land Surface Temperature TNT MODIS Terra Nighttime Land Surface Temperature","title":"VAM parameter codes"},{"location":"#temporal-interpolation-codes","text":"Code Reference TXN Native temporal resolution (no interpolation performed) TXD 10-day timestep (dekads) TXP 5-day timestep (pentads) TXC Custom interpolation (as defined by user)","title":"Temporal interpolation codes"},{"location":"#examples","text":"","title":"Examples"},{"location":"#raw-hdf5-file","text":"Example MOD13A2.h20v08.006.VIM.h5 : this is an example for 1km MODIS NDVI, collection 6, for the tile h20v08 from the Terra satellite MXD13A2.h20v08.006.VIM.h5 : if both Aqua and Terra get interleaved, the product code changes to MXD MXD13C1.006.VIM.h5 : if the product is global, the filename does not include a tile index","title":"Raw HDF5 file"},{"location":"#smooth-hdf5-file","text":"The filename for a smooth HDF5 file is directly derived from the raw input, so all elements of the raw file will be adopted. The only thing changing is the added information of the temporal interpolation. Example MXD13A2.h20v08.006.txn.VIM.h5 : smooth version of the rawfile above, without temporal interpolation (so in native timestep) MXD13A2.h20v08.006.txd.VIM.h5 : same as above, just with temporal interpolation performed to 10-day (dekad) timestep","title":"Smooth HDF5 file"},{"location":"#exported-geotiffs","text":"The naming for the exported GeoTIFFs is put together from the region code (supplied by the user - if not, the default is reg ), VAM code and the timestamp (either in julian format or dekads/pentads if applicable). Note The filenames don't contain any information about the product or tile, so GeoTIFFs from different files can end up having the same name and be easily overwritten. Caution is advised! Example regvim2020001.tif : basic example for an exported NDVI image for either a raw HDF5 file, or a smooth HDF5 file with no or custom temporal interpolation regvim2020d1.tif : If the temporal interpolation is dekad, the timestamps are converted to dekads (this behavior can be prevented with a flag in the executable)","title":"Exported GeoTIFFs"},{"location":"collect/","text":"ModisRawH5 Warning only implemented for MOD/MYD 11/13 products! Class representing HDF5 file containing raw MODIS data. This class will create an HDF5 file and collect data from MODIS HDF files into it. This file can then be used for smoothing in a subsequent step. __init__ ( self , files , targetdir , vam_product_code = None , interleave = False ) special Initialize instance ModisRawH5 class. This creates an ModisRawH5 object. If the corresponding HDF5 file already exists, it'll be automatically linked to it. If not, the file will be created on the first update run. All HDF files in the files list will be collected. The user needs to be aware that temporal consistency is conserved! To make sure the update workflow is functioning as intended, it's important that targetdir is set correctly. This way existing HDF5 files can be updated, and new ones created. To select a specific subdataset, vam_product_code needs to be provided. If not, the defaults will be extracted (VIM / TDA/ TDT). For VIM, 16 day composite products can be interleaved to form a synthetic 8 day product if both satellites (MOD & MYD) are present in files . The resulting HDF5 file will be named with MXD as product code. Parameters: Name Type Description Default files List[str] A list of absolute paths to MODIS raw hdf files to be processed required vam_product_code str VAM product code to be processed (default VIM/LTD) None targetdir str Target directory for raw MODIS HDF5 file required interleave bool Boolean flag if MOD/MYD 13 products should be interleaved False Exceptions: Type Description ValueError If other product than MOD/MYD 11/13 are provided. AssertionError If files from multiple products are provided (except interleave). AssertionError If duplicates are detected that can't be handled. AssertionError If vam_product_code is not supported. create ( self , compression = 'gzip' , chunks = None ) Creates HDF5 file. If the corresponding HDF5 is not found in the target directory, it's created. If no chunking scheme is specified using chunks , a generic one of (number rows // 25, 1) will be used where the rows represent the spatial dimension and the colums the temporal dimension. Parameters: Name Type Description Default compression str Compression for data (default = gzip). 'gzip' chunks Tuple[int] Chunksize for data (tuple of 2 int; default = (rows//25, 1)). None Exceptions: Type Description AssertionError If chunks is not a Tuple containing 2 int . HDF5CreationError If creation of HDF5 file fails. update ( self ) Updates MODIS raw HDF5 file with raw data. The files specified in __init__ get collected into the HDF5 file, which is either created before or already existed after a previous initialization. If a HDF file can't be read, the datapoints are filled using the internal nodata value. Exceptions: Type Description AssertionError If dates of files to be collected are not after the ones aleady contained in the file. HDF5WriteError If writing to the HDF5 file fails.","title":"modis.collect"},{"location":"collect/#modisrawh5","text":"Warning only implemented for MOD/MYD 11/13 products! Class representing HDF5 file containing raw MODIS data. This class will create an HDF5 file and collect data from MODIS HDF files into it. This file can then be used for smoothing in a subsequent step.","title":"ModisRawH5"},{"location":"collect/#modape.modis.collect.ModisRawH5.__init__","text":"Initialize instance ModisRawH5 class. This creates an ModisRawH5 object. If the corresponding HDF5 file already exists, it'll be automatically linked to it. If not, the file will be created on the first update run. All HDF files in the files list will be collected. The user needs to be aware that temporal consistency is conserved! To make sure the update workflow is functioning as intended, it's important that targetdir is set correctly. This way existing HDF5 files can be updated, and new ones created. To select a specific subdataset, vam_product_code needs to be provided. If not, the defaults will be extracted (VIM / TDA/ TDT). For VIM, 16 day composite products can be interleaved to form a synthetic 8 day product if both satellites (MOD & MYD) are present in files . The resulting HDF5 file will be named with MXD as product code. Parameters: Name Type Description Default files List[str] A list of absolute paths to MODIS raw hdf files to be processed required vam_product_code str VAM product code to be processed (default VIM/LTD) None targetdir str Target directory for raw MODIS HDF5 file required interleave bool Boolean flag if MOD/MYD 13 products should be interleaved False Exceptions: Type Description ValueError If other product than MOD/MYD 11/13 are provided. AssertionError If files from multiple products are provided (except interleave). AssertionError If duplicates are detected that can't be handled. AssertionError If vam_product_code is not supported.","title":"__init__()"},{"location":"collect/#modape.modis.collect.ModisRawH5.create","text":"Creates HDF5 file. If the corresponding HDF5 is not found in the target directory, it's created. If no chunking scheme is specified using chunks , a generic one of (number rows // 25, 1) will be used where the rows represent the spatial dimension and the colums the temporal dimension. Parameters: Name Type Description Default compression str Compression for data (default = gzip). 'gzip' chunks Tuple[int] Chunksize for data (tuple of 2 int; default = (rows//25, 1)). None Exceptions: Type Description AssertionError If chunks is not a Tuple containing 2 int . HDF5CreationError If creation of HDF5 file fails.","title":"create()"},{"location":"collect/#modape.modis.collect.ModisRawH5.update","text":"Updates MODIS raw HDF5 file with raw data. The files specified in __init__ get collected into the HDF5 file, which is either created before or already existed after a previous initialization. If a HDF file can't be read, the datapoints are filled using the internal nodata value. Exceptions: Type Description AssertionError If dates of files to be collected are not after the ones aleady contained in the file. HDF5WriteError If writing to the HDF5 file fails.","title":"update()"},{"location":"download/","text":"ModisQuery Class for querying and downloading MODIS data. The user can create a query and send it to NASA's CMR servers. The response can be either just printed to console or passed to the download method, to fetch the resulting HDF files to local disk. __init__ ( self , products , aoi = None , begindate = None , enddate = None , tile_filter = None , version = '006' ) special Initialize instance ModisQuery class. This creates an instance of ModisQuery with the basic query parameters. The aoi needs to be a list if either 2 or 4 coordinates as float or int , depending on if a point or bounding box is requested. The tile_filter needs to be specified as list of MODIS tile IDs in format hXXvXX (e.g. h20v08 ). Parameters: Name Type Description Default products List[str] List of product codes to be queried / downloaded. required aoi List[Union[float, int]] Area of interes (point as lat/lon or bounding box as xmin, ymin, xmax, ymax). None begindate datetime Start date for query. None enddate datetime End date for query. None tile_filter List[str] List of tiles to be queried / downloaded (refines initial results). None version str MODIS collection version. '006' Exceptions: Type Description AssertionError If no product code is supplied or if len(aoi) not in [2, 4] . download ( self , targetdir , username , password , overwrite = False , multithread = False , nthreads = 4 ) Download MODIS HDF files. This method downloads the MODIS HDF files contained in the server response to the search call. This requires NASA earthdata credentials. To speed up download, multiple threads can be used. Parameters: Name Type Description Default targetdir Path Target directory for files being downloaded. required username str Earthdata username. required password str Earthdata password. required overwrite bool Replace existing files. False multithread bool Use multiple threads for downloading. False nthreads int Number of threads. 4 Exceptions: Type Description DownloadError If one or more errors were encountered during downloading. search ( self , strict_dates = True ) Send quert to MODIS CMR servers. Constructs the query from parameters passed to __init__ and sends the query to the NASA servers. The returned results will be stored in a class variable. To deal with overlapping date ranges of composite products, the specified start and end date can be strictly enforced. Parameters: Name Type Description Default strict_dates bool Flag for strict date enforcement (no data with timestamp outside of begindate and enddate are allowed). True","title":"modis.download"},{"location":"download/#modisquery","text":"Class for querying and downloading MODIS data. The user can create a query and send it to NASA's CMR servers. The response can be either just printed to console or passed to the download method, to fetch the resulting HDF files to local disk.","title":"ModisQuery"},{"location":"download/#modape.modis.download.ModisQuery.__init__","text":"Initialize instance ModisQuery class. This creates an instance of ModisQuery with the basic query parameters. The aoi needs to be a list if either 2 or 4 coordinates as float or int , depending on if a point or bounding box is requested. The tile_filter needs to be specified as list of MODIS tile IDs in format hXXvXX (e.g. h20v08 ). Parameters: Name Type Description Default products List[str] List of product codes to be queried / downloaded. required aoi List[Union[float, int]] Area of interes (point as lat/lon or bounding box as xmin, ymin, xmax, ymax). None begindate datetime Start date for query. None enddate datetime End date for query. None tile_filter List[str] List of tiles to be queried / downloaded (refines initial results). None version str MODIS collection version. '006' Exceptions: Type Description AssertionError If no product code is supplied or if len(aoi) not in [2, 4] .","title":"__init__()"},{"location":"download/#modape.modis.download.ModisQuery.download","text":"Download MODIS HDF files. This method downloads the MODIS HDF files contained in the server response to the search call. This requires NASA earthdata credentials. To speed up download, multiple threads can be used. Parameters: Name Type Description Default targetdir Path Target directory for files being downloaded. required username str Earthdata username. required password str Earthdata password. required overwrite bool Replace existing files. False multithread bool Use multiple threads for downloading. False nthreads int Number of threads. 4 Exceptions: Type Description DownloadError If one or more errors were encountered during downloading.","title":"download()"},{"location":"download/#modape.modis.download.ModisQuery.search","text":"Send quert to MODIS CMR servers. Constructs the query from parameters passed to __init__ and sends the query to the NASA servers. The returned results will be stored in a class variable. To deal with overlapping date ranges of composite products, the specified start and end date can be strictly enforced. Parameters: Name Type Description Default strict_dates bool Flag for strict date enforcement (no data with timestamp outside of begindate and enddate are allowed). True","title":"search()"},{"location":"io/","text":"HDF5Base Parent class for interaction with HDF5 files This class serves as a parent class for ModisRawH5 and ModisSmoothH5, enabling uniform chunked read and write of datasets and attributes to and from HDF5 files. __init__ ( self , filename ) special Initialize HDF5Base instance. Creates an instance of the HDF5Base class. This is not stricly intended to be called by itself (although it can be), but rather through a super call of the child class. Parameters: Name Type Description Default filename str Full path to the HDF5 file. required read_chunked ( self , dataset , xoffset = 0 , xchunk = None , arr_out = None ) Read data from dataset in a chunked manner. The chunks are iterated in a row by column pattern, where each chunk along row axis is yielded once the full column size is read into the array. The chunking of the colums ( xchunk ) can me modified, while the row chunking ( ychunk ) is strictly defined by the dataset. To enable the nsmooth functionality, an xoffset can be provided to skip datapoints along the time dimension. If no arr_out is provided, a new array will be created with the necessary dimensions. Parameters: Name Type Description Default dataset str Name of the dataset (expect 2d array). required xoffset int Offset for time-dimension (xaxis) in file. 0 xchunk int Chunking along time-dimension. If not specified, it'll be read from the dataset. None arr_out ndarray Output array. None Returns: Type Description ndarray Yields the output chunk as np.ndarray Exceptions: Type Description AssertionError Raised when dataset not found within HDF5 file and when provided arr_out is not correct object or shape write_chunk ( self , dataset , arr_in , xoffset = 0 , xchunk = None , yoffset = 0 ) Write chunk back to HDF5 file. Writes complete chunk back to HDF5 file, iterating over the time-dimension (x). The chunksize for x can be adjusted manually using xchunk . To implement nupdate behaviour, xoffset can be used to skip prior datapoints in the time-dimension. To write successive spatial chunks, yoffset has to be provided (the default is 0, as it starts at the top left). Parameters: Name Type Description Default dataset str Name of the dataset (expect 2d array). required arr_in ndarray Input data to be written to file. required xchunk int Chunking along row (x) axis. If not specified, it'll be read from the dataset. None xoffset int Offset for colums (xaxis) in file. 0 yoffset int Offset for rows (yaxis) in file. 0 Returns: Type Description bool Returns True if write was successful. Exceptions: Type Description AssertionError Raised when dataset not found within HDF5 file and when provided arr_in is not correct object or shape HDFHandler Class to handle reading from MODIS HDF files. This class enables reading specific subdatasets and attributes from the raw MODIS HDF files. iter_handles ( self ) Iterates over all open dataset handles coming from open_datasets and returns a Tuple with index and a gdal.Dataset for each. Returns: Type Description Tuple[int, gdal.Dataset] Tuple with index and corresponding gdal.Dataset open_datasets ( self ) Opens the selected subdataset from all files within a context manager and stores them in a class variable. When the context manager closes, the refereces are removed, closing all datasets. read_chunk ( x , ** kwargs ) staticmethod Reads a chunk of an opened subdataset. The size of the chunk being read is defined by the values passed to gdal.Dataset.ReadAsArray with kwargs and can be as big as the entire dataset. Parameters: Name Type Description Default x gdal.Dataset GDAL Dataset. required **kwargs dict kwargs passed on to gdal.Dataset.ReadAsArray {} Returns: Type Description ndarray Requested chunk as np.ndarray","title":"modis.io"},{"location":"io/#hdf5base","text":"Parent class for interaction with HDF5 files This class serves as a parent class for ModisRawH5 and ModisSmoothH5, enabling uniform chunked read and write of datasets and attributes to and from HDF5 files.","title":"HDF5Base"},{"location":"io/#modape.modis.io.HDF5Base.__init__","text":"Initialize HDF5Base instance. Creates an instance of the HDF5Base class. This is not stricly intended to be called by itself (although it can be), but rather through a super call of the child class. Parameters: Name Type Description Default filename str Full path to the HDF5 file. required","title":"__init__()"},{"location":"io/#modape.modis.io.HDF5Base.read_chunked","text":"Read data from dataset in a chunked manner. The chunks are iterated in a row by column pattern, where each chunk along row axis is yielded once the full column size is read into the array. The chunking of the colums ( xchunk ) can me modified, while the row chunking ( ychunk ) is strictly defined by the dataset. To enable the nsmooth functionality, an xoffset can be provided to skip datapoints along the time dimension. If no arr_out is provided, a new array will be created with the necessary dimensions. Parameters: Name Type Description Default dataset str Name of the dataset (expect 2d array). required xoffset int Offset for time-dimension (xaxis) in file. 0 xchunk int Chunking along time-dimension. If not specified, it'll be read from the dataset. None arr_out ndarray Output array. None Returns: Type Description ndarray Yields the output chunk as np.ndarray Exceptions: Type Description AssertionError Raised when dataset not found within HDF5 file and when provided arr_out is not correct object or shape","title":"read_chunked()"},{"location":"io/#modape.modis.io.HDF5Base.write_chunk","text":"Write chunk back to HDF5 file. Writes complete chunk back to HDF5 file, iterating over the time-dimension (x). The chunksize for x can be adjusted manually using xchunk . To implement nupdate behaviour, xoffset can be used to skip prior datapoints in the time-dimension. To write successive spatial chunks, yoffset has to be provided (the default is 0, as it starts at the top left). Parameters: Name Type Description Default dataset str Name of the dataset (expect 2d array). required arr_in ndarray Input data to be written to file. required xchunk int Chunking along row (x) axis. If not specified, it'll be read from the dataset. None xoffset int Offset for colums (xaxis) in file. 0 yoffset int Offset for rows (yaxis) in file. 0 Returns: Type Description bool Returns True if write was successful. Exceptions: Type Description AssertionError Raised when dataset not found within HDF5 file and when provided arr_in is not correct object or shape","title":"write_chunk()"},{"location":"io/#hdfhandler","text":"Class to handle reading from MODIS HDF files. This class enables reading specific subdatasets and attributes from the raw MODIS HDF files.","title":"HDFHandler"},{"location":"io/#modape.modis.io.HDFHandler.iter_handles","text":"Iterates over all open dataset handles coming from open_datasets and returns a Tuple with index and a gdal.Dataset for each. Returns: Type Description Tuple[int, gdal.Dataset] Tuple with index and corresponding gdal.Dataset","title":"iter_handles()"},{"location":"io/#modape.modis.io.HDFHandler.open_datasets","text":"Opens the selected subdataset from all files within a context manager and stores them in a class variable. When the context manager closes, the refereces are removed, closing all datasets.","title":"open_datasets()"},{"location":"io/#modape.modis.io.HDFHandler.read_chunk","text":"Reads a chunk of an opened subdataset. The size of the chunk being read is defined by the values passed to gdal.Dataset.ReadAsArray with kwargs and can be as big as the entire dataset. Parameters: Name Type Description Default x gdal.Dataset GDAL Dataset. required **kwargs dict kwargs passed on to gdal.Dataset.ReadAsArray {} Returns: Type Description ndarray Requested chunk as np.ndarray","title":"read_chunk()"},{"location":"modis_executables/","text":"The MODIS processing chain can be executed at a high level using CLI scrips with minimal user input. All of the scripts are created using the Python module click . Warning modis_collect , modis_smooth and modis_window are only implemented for MOD/MYD 11/13 products! modis_download With modis_download you can query and download MODIS raw data. Note Downloading NASA data to local disk requires valid Earthdata credentials . The products to be queried for need to be supplied as a list of MODIS product IDs (case insensitive), each item separated by a single space, e.g. MOD13A2 MYD11A1 ... . Tip If you want to query for both MODIS satellites for the same product, the letter indicating which satellite can be replaced by a ? , e.g. M?d13A2 . The query can be limited by coordinates (as --roi point or bounding box), MODIS tile IDs (supplied as comma separated list to --tile-filter , e.g h20v08,h20v09 ) and a date bracket (supplied to --begin-date and --end-date ). Due to the nature of composited products, results can be returned slightly outside the specified date bracket. To avoid this and strictly enforce the bracket, pass the --strict-dates flag. The query is sent to the NASA Common Metadata Repository (CMR) using the python-cmr module. Tip To look at the details of the returned results, pass the --return-results flag. The download is only performed when both credential flags are passed ( --username & --password ) as well as the --download flag. By default, the download is performed sequentially using Python's requests . Tip To speed up the download, you may use multiple threads ( --multithread ) where the number of used threads can be adjusted with the --nthreads flag (default is 4 threads). A processing chain might require the target directory to not contain any previous HDF files, which can be enforced using --target-empty . By default, existing files won't be overwritten. If this is required, pass the --overwrite flag. Usage Usage: modis_download [OPTIONS] [PRODUCTS]... Options: --roi TEXT Region of interest. Either LAT,LON or xmin,ymin,xmax,ymax -b, --begin-date [%Y-%m-%d] Start date for query -e, --end-date [%Y-%m-%d] End date for query -d, --targetdir PATH Destination directory for downloaded files --target-empty Fail if there are hdf files in the target directory --tile-filter TEXT Filter tiles - supplied as csv list --username TEXT Earthdata username --password TEXT Earthdata password --strict-dates Don't allow files with timestamps outside of provided date(s) --return-results Print results to console --download Download data --overwrite Overwrite existing files --multithread Use multiple threads for downloading --nthreads INTEGER Number of threads to use -c, --collection TEXT MODIS collection --help Show this message and exit. modis_collect Before applying the Whittake smoother to the data, the raw HDF files need to be collected into a 3D spatio-temporal cube inside an HDF5 file, using modis_collect . The only required input is src_dir , a directory with the raw MODIS HDF files to be collected. These can be multiple products and tiles. If no target directory ( --target-dir ) is specified, the output will be stored in the src_dir . The HDF5 files themselves get stored in a sub-directory, which is attributed to the extracted parameter (indicated by the VAM parameter code). This needs to be considered carefully, as updating the existing files depends on the executable being able to locate them. To extract a specific parameter, a VAM parameter code can be specified using --vam-code . If not specified, the defaults will be extracted, which are NDVI (VIM) and LST Daytime (TDA/TDT). In the case of 16 day MODIS NDVI products, both satellites can be interleaved to form a combined 8-day product, indicated by a new MXD product code. Each combination of MODIS product and tile get collected into the same HDF5 file. Tip Using --parallel-tiles , these can be collected in parallel (up to 4 collection processes). If an HDF5 file does not exist, it will be created, otherwise updated with the new data. It might be required to perform a check on temporal continuity, which can be done using --last-collected . Here a MODIS julian date can be specified that needs to be the last timestep collected before the new collection starts. If that's not the case, the process will fail on exception. Danger It's the user's responsibility to ensure temporal continuity when creating and updating files. New timesteps are simply appended, only checking if they don't precede existing ones. There are no further checks if a timestep might be missing etc. This follows the \"garbage in - garbage out\" principle. Usage Usage: modis_collect [OPTIONS] SRC_DIR Options: -d, --targetdir PATH Destination for raw HDF5 files -x, --compression TEXT Compression for HDF5 files --vam-code TEXT VAM code for dataset to process --interleave Interleave MOD13 & MYD13 products to MXD (only works for VIM!) --parallel-tiles INTEGER Number of tiles processed in parallel (default = None) --cleanup Remove collected HDF files --last-collected [%Y%j] Last collected date in julian format (YYYYDDD - %Y%j) --help Show this message and exit. modis_smooth This step applies the Whittaker smoother to the raw data. The input ( src ) can be either a single raw HDF5 file, or a directory containing multiple. There's a one-to-many relationship between raw and smooth HDF5 files. One raw file can have multiple smooth files, depending on the settings used for smoothing. As with the raw HDF5 files, the smooth files get created if they don't exist or updated if they do. So it's again crucial to be aware of where the data gets written to, and adjust accordingly with --targetdir . The smoothing can be performed in multiple ways, which is determined by the user's input: if the --soptimize flag is passed, the V-curve optimization of the smoothing parameter S is performed. This always takes precedence over other methods, independent of other inputs supplied. if --soptimize is passed alongside a P value ( --pvalue ), then an asymmetric V-curve optimization is performed using the supplied P value. if only the S Value ( --svalue ) is supplied, then the smoothing is performed using a fixed S (as supplied - should be in log10) if none of the above is supplied, the smoothing will be performed reading S values from a previously optimized S-grid. Note: This requires that --soptimize has been run at least once previously. The V-curve optimization will look for an optimized S value for each pixel within a given range. This range can be defined using the --srange flag (expects 3 space separated log10 values). If the range is not supplied, the algorithm will use one of two for each pixel, depending on the lag-1 correlation of the pixel's timeseries: lag-1 correlation > 0.5: smin=-2 , smax=1 , sstep=0.2 lag-1 correlation <= 0.5: smin=0 , smax=3 , sstep=0.2 The smoother can also perform temporal interpolation, to a desired temporal grid. The desired timestep should be defined with --tempint . Dekad (10-day) and pentad (5-day) interpolation follow a fixed date cycle. For other more custom interpolations, it's required to specify a start date ( --tempint-start ) from which the timestep will be calculated. For subsequent runs after optimization, the number of timesteps used for smoothing ( --nsmooth ) and the number updated in the target dataset ( --nupdate ) can be limited to speed up the process. Similar to raw HDF5 files, the --last-collected flag can be passed to enforce a check on the last raw date used for smoothing in the previous run. Usage Usage: modis_smooth [OPTIONS] SRC Options: -d, --targetdir PATH Target directory for smoothed output -s, --svalue FLOAT S value for smoothing (in log10) -S, --srange FLOAT... S value range for V-curve (float log10(s) values as smin smax sstep - default -1 1 0) -p, --pvalue FLOAT P value for asymmetric smoothing -t, --tempint INTEGER Value for temporal interpolation (integer required - default is native temporal resolution i.e. no interpolation) --tempint-start [%Y-%m-%d|%Y%j] Startdate for temporal interpolation -n, --nsmooth INTEGER Number of raw timesteps used for smoothing -u, --nupdate INTEGER Number of smoothed timesteps to be updated in HDF5 file --soptimize Use V-curve for s value optimization --parallel-tiles INTEGER Number of tiles processed in parallel --last-collected [%Y%j] Last collected date in julian format (YYYYDDD - %Y%j) --help Show this message and exit. modis_window modis_window enables the export of data from HDF5 files as GeoTIFF. The data can be a mosaic of many tiles, or a subset of a single tile / global file. The input ( src ) can be a single HDF5 file or a directory containing multiple files. In the second case, the input can be filtered by MODIS product ( --filter-product ) and VAM parameter code ( --filter-vampc ). For subsets, a region of interest ( --roi ) needs to be specified as comma separated list. The exported dates can be limited by passing a date bracket. ( --begin-date & --end-date ). In case of smooth HDF5 files, the grid containing the V-curve optimized S values for each pixel can be exported by passing the --sgrid flag. In this case any input dates won't have an effect (there's only one S-grid). If outputs in the MODIS Sinusoidal projection are warped by default to EPSG:4326 . For warping to a different spatial reference, specify a GDAL readable reference to --target-srs . Note Only tiled MODIS products are in Sinusoidal projection. The global 5km products are already in EPSG:4326 and don't need to be warped. By default, all exported GeoTIFFs get compressed using LZW with predictor 2 (using the creation options [\"COMPRESS=LZW\", \"PREDICTOR=2\"] ). To specify different GDAL creation options, a space separated list in form of KEY=VALUE can be passed to --co . Tip To further improve the compression and reduce file size, the data can be rounded to a significant integer. E.g. a NDVI value of 3141 can be rounded to 3100 by passing 2 to --round-int . Tip For further customization of the exported GeoTIFFs, additional GDAL options can be supplied to --gdal-kwarg in the same format. The file naming for exported GeoTIFFs is detailed in the main documentation . A custom region code can be specified using --region . When exporting dekad / pentad data, --force-doy can be used to force a YYYYjDDD timestamp instead of the dekad / pentad format. Note Note: please read the section in the main documentation carefully to avoid confusion with identical named GeoTIFFs. By default existing GeoTIFFs won't be overwritten. This can be forced with passwing the --overwrite flag. Usage Usage: modis_window [OPTIONS] SRC Options: -d, --targetdir PATH Target directory for Tiffs -b, --begin-date [%Y-%m-%d] Begin date for Tiffs -e, --end-date [%Y-%m-%d] End date for Tiffs --roi TEXT AOI for clipping (as ULX,ULY,LRX,LRY) --region TEXT Region prefix for Tiffs (default is reg) --sgrid Extract sgrid instead of data --force-doy Force DOY filenaming --filter-product TEXT Filter by product --filter-vampc TEXT Filter by VAM parameter code --target-srs TEXT Target spatial reference for warping --co TEXT GDAL creationOptions --clip-valid clip values to valid range for product --round-int INTEGER Round to integer places (either decimals or exponent of 10) --gdal-kwarg TEXT Addition kwargs for GDAL in form KEY=VALUE (multiple allowed) --overwrite Overwrite existsing Tiffs --help Show this message and exit.","title":"MODIS processing chain"},{"location":"modis_executables/#modis_download","text":"With modis_download you can query and download MODIS raw data. Note Downloading NASA data to local disk requires valid Earthdata credentials . The products to be queried for need to be supplied as a list of MODIS product IDs (case insensitive), each item separated by a single space, e.g. MOD13A2 MYD11A1 ... . Tip If you want to query for both MODIS satellites for the same product, the letter indicating which satellite can be replaced by a ? , e.g. M?d13A2 . The query can be limited by coordinates (as --roi point or bounding box), MODIS tile IDs (supplied as comma separated list to --tile-filter , e.g h20v08,h20v09 ) and a date bracket (supplied to --begin-date and --end-date ). Due to the nature of composited products, results can be returned slightly outside the specified date bracket. To avoid this and strictly enforce the bracket, pass the --strict-dates flag. The query is sent to the NASA Common Metadata Repository (CMR) using the python-cmr module. Tip To look at the details of the returned results, pass the --return-results flag. The download is only performed when both credential flags are passed ( --username & --password ) as well as the --download flag. By default, the download is performed sequentially using Python's requests . Tip To speed up the download, you may use multiple threads ( --multithread ) where the number of used threads can be adjusted with the --nthreads flag (default is 4 threads). A processing chain might require the target directory to not contain any previous HDF files, which can be enforced using --target-empty . By default, existing files won't be overwritten. If this is required, pass the --overwrite flag.","title":"modis_download"},{"location":"modis_executables/#usage","text":"Usage: modis_download [OPTIONS] [PRODUCTS]... Options: --roi TEXT Region of interest. Either LAT,LON or xmin,ymin,xmax,ymax -b, --begin-date [%Y-%m-%d] Start date for query -e, --end-date [%Y-%m-%d] End date for query -d, --targetdir PATH Destination directory for downloaded files --target-empty Fail if there are hdf files in the target directory --tile-filter TEXT Filter tiles - supplied as csv list --username TEXT Earthdata username --password TEXT Earthdata password --strict-dates Don't allow files with timestamps outside of provided date(s) --return-results Print results to console --download Download data --overwrite Overwrite existing files --multithread Use multiple threads for downloading --nthreads INTEGER Number of threads to use -c, --collection TEXT MODIS collection --help Show this message and exit.","title":"Usage"},{"location":"modis_executables/#modis_collect","text":"Before applying the Whittake smoother to the data, the raw HDF files need to be collected into a 3D spatio-temporal cube inside an HDF5 file, using modis_collect . The only required input is src_dir , a directory with the raw MODIS HDF files to be collected. These can be multiple products and tiles. If no target directory ( --target-dir ) is specified, the output will be stored in the src_dir . The HDF5 files themselves get stored in a sub-directory, which is attributed to the extracted parameter (indicated by the VAM parameter code). This needs to be considered carefully, as updating the existing files depends on the executable being able to locate them. To extract a specific parameter, a VAM parameter code can be specified using --vam-code . If not specified, the defaults will be extracted, which are NDVI (VIM) and LST Daytime (TDA/TDT). In the case of 16 day MODIS NDVI products, both satellites can be interleaved to form a combined 8-day product, indicated by a new MXD product code. Each combination of MODIS product and tile get collected into the same HDF5 file. Tip Using --parallel-tiles , these can be collected in parallel (up to 4 collection processes). If an HDF5 file does not exist, it will be created, otherwise updated with the new data. It might be required to perform a check on temporal continuity, which can be done using --last-collected . Here a MODIS julian date can be specified that needs to be the last timestep collected before the new collection starts. If that's not the case, the process will fail on exception. Danger It's the user's responsibility to ensure temporal continuity when creating and updating files. New timesteps are simply appended, only checking if they don't precede existing ones. There are no further checks if a timestep might be missing etc. This follows the \"garbage in - garbage out\" principle.","title":"modis_collect"},{"location":"modis_executables/#usage_1","text":"Usage: modis_collect [OPTIONS] SRC_DIR Options: -d, --targetdir PATH Destination for raw HDF5 files -x, --compression TEXT Compression for HDF5 files --vam-code TEXT VAM code for dataset to process --interleave Interleave MOD13 & MYD13 products to MXD (only works for VIM!) --parallel-tiles INTEGER Number of tiles processed in parallel (default = None) --cleanup Remove collected HDF files --last-collected [%Y%j] Last collected date in julian format (YYYYDDD - %Y%j) --help Show this message and exit.","title":"Usage"},{"location":"modis_executables/#modis_smooth","text":"This step applies the Whittaker smoother to the raw data. The input ( src ) can be either a single raw HDF5 file, or a directory containing multiple. There's a one-to-many relationship between raw and smooth HDF5 files. One raw file can have multiple smooth files, depending on the settings used for smoothing. As with the raw HDF5 files, the smooth files get created if they don't exist or updated if they do. So it's again crucial to be aware of where the data gets written to, and adjust accordingly with --targetdir . The smoothing can be performed in multiple ways, which is determined by the user's input: if the --soptimize flag is passed, the V-curve optimization of the smoothing parameter S is performed. This always takes precedence over other methods, independent of other inputs supplied. if --soptimize is passed alongside a P value ( --pvalue ), then an asymmetric V-curve optimization is performed using the supplied P value. if only the S Value ( --svalue ) is supplied, then the smoothing is performed using a fixed S (as supplied - should be in log10) if none of the above is supplied, the smoothing will be performed reading S values from a previously optimized S-grid. Note: This requires that --soptimize has been run at least once previously. The V-curve optimization will look for an optimized S value for each pixel within a given range. This range can be defined using the --srange flag (expects 3 space separated log10 values). If the range is not supplied, the algorithm will use one of two for each pixel, depending on the lag-1 correlation of the pixel's timeseries: lag-1 correlation > 0.5: smin=-2 , smax=1 , sstep=0.2 lag-1 correlation <= 0.5: smin=0 , smax=3 , sstep=0.2 The smoother can also perform temporal interpolation, to a desired temporal grid. The desired timestep should be defined with --tempint . Dekad (10-day) and pentad (5-day) interpolation follow a fixed date cycle. For other more custom interpolations, it's required to specify a start date ( --tempint-start ) from which the timestep will be calculated. For subsequent runs after optimization, the number of timesteps used for smoothing ( --nsmooth ) and the number updated in the target dataset ( --nupdate ) can be limited to speed up the process. Similar to raw HDF5 files, the --last-collected flag can be passed to enforce a check on the last raw date used for smoothing in the previous run.","title":"modis_smooth"},{"location":"modis_executables/#usage_2","text":"Usage: modis_smooth [OPTIONS] SRC Options: -d, --targetdir PATH Target directory for smoothed output -s, --svalue FLOAT S value for smoothing (in log10) -S, --srange FLOAT... S value range for V-curve (float log10(s) values as smin smax sstep - default -1 1 0) -p, --pvalue FLOAT P value for asymmetric smoothing -t, --tempint INTEGER Value for temporal interpolation (integer required - default is native temporal resolution i.e. no interpolation) --tempint-start [%Y-%m-%d|%Y%j] Startdate for temporal interpolation -n, --nsmooth INTEGER Number of raw timesteps used for smoothing -u, --nupdate INTEGER Number of smoothed timesteps to be updated in HDF5 file --soptimize Use V-curve for s value optimization --parallel-tiles INTEGER Number of tiles processed in parallel --last-collected [%Y%j] Last collected date in julian format (YYYYDDD - %Y%j) --help Show this message and exit.","title":"Usage"},{"location":"modis_executables/#modis_window","text":"modis_window enables the export of data from HDF5 files as GeoTIFF. The data can be a mosaic of many tiles, or a subset of a single tile / global file. The input ( src ) can be a single HDF5 file or a directory containing multiple files. In the second case, the input can be filtered by MODIS product ( --filter-product ) and VAM parameter code ( --filter-vampc ). For subsets, a region of interest ( --roi ) needs to be specified as comma separated list. The exported dates can be limited by passing a date bracket. ( --begin-date & --end-date ). In case of smooth HDF5 files, the grid containing the V-curve optimized S values for each pixel can be exported by passing the --sgrid flag. In this case any input dates won't have an effect (there's only one S-grid). If outputs in the MODIS Sinusoidal projection are warped by default to EPSG:4326 . For warping to a different spatial reference, specify a GDAL readable reference to --target-srs . Note Only tiled MODIS products are in Sinusoidal projection. The global 5km products are already in EPSG:4326 and don't need to be warped. By default, all exported GeoTIFFs get compressed using LZW with predictor 2 (using the creation options [\"COMPRESS=LZW\", \"PREDICTOR=2\"] ). To specify different GDAL creation options, a space separated list in form of KEY=VALUE can be passed to --co . Tip To further improve the compression and reduce file size, the data can be rounded to a significant integer. E.g. a NDVI value of 3141 can be rounded to 3100 by passing 2 to --round-int . Tip For further customization of the exported GeoTIFFs, additional GDAL options can be supplied to --gdal-kwarg in the same format. The file naming for exported GeoTIFFs is detailed in the main documentation . A custom region code can be specified using --region . When exporting dekad / pentad data, --force-doy can be used to force a YYYYjDDD timestamp instead of the dekad / pentad format. Note Note: please read the section in the main documentation carefully to avoid confusion with identical named GeoTIFFs. By default existing GeoTIFFs won't be overwritten. This can be forced with passwing the --overwrite flag.","title":"modis_window"},{"location":"modis_executables/#usage_3","text":"Usage: modis_window [OPTIONS] SRC Options: -d, --targetdir PATH Target directory for Tiffs -b, --begin-date [%Y-%m-%d] Begin date for Tiffs -e, --end-date [%Y-%m-%d] End date for Tiffs --roi TEXT AOI for clipping (as ULX,ULY,LRX,LRY) --region TEXT Region prefix for Tiffs (default is reg) --sgrid Extract sgrid instead of data --force-doy Force DOY filenaming --filter-product TEXT Filter by product --filter-vampc TEXT Filter by VAM parameter code --target-srs TEXT Target spatial reference for warping --co TEXT GDAL creationOptions --clip-valid clip values to valid range for product --round-int INTEGER Round to integer places (either decimals or exponent of 10) --gdal-kwarg TEXT Addition kwargs for GDAL in form KEY=VALUE (multiple allowed) --overwrite Overwrite existsing Tiffs --help Show this message and exit.","title":"Usage"},{"location":"other_executables/","text":"Additional commandline scripts which are not strictly part of the MODIS processing chain. csv_smooth This executable allows to smooth timeseries saved in CSV format. Note For the different ways the smoother can be applied, please refer to the modis_smooth documentation. csv_smooth expects the timeseries for smoothing to be stored in the columns, therefore each column will be smoothed independently. If a column with dates or another index is present, specify it in --index-column . Header rows can be skipped using the --skip-header-rows flag. If there are column names or other metainfo (e.g. coordinates) that should not be filtered, the index of the first data cell can be specified using --data-start . Usage Usage: csv_smooth [OPTIONS] CSV_FILE Options: -s, --svalue FLOAT S value for smoothing (has to be log10(s)) -S, --srange FLOAT... S value range for V-curve (float log10(s) values as smin smax sstep - default 0 4 0.1) -p, --pvalue FLOAT P value for asymmetric smoothing -n, --nodata FLOAT nodata value --skip-header-rows INTEGER Number of header rows to skip --data-start INTEGER Number of row with first datapoint --index-column INTEGER Index column number --soptimize Use V-curve for s value optimization --help Show this message and exit. modis_info This executable can be used to inspect the raw/smooth HDF5 files. When supplied with the path to a file, a selection of metadata is printed to console, such as dimensions, begin/end date, last smoothing run (for smooth files), ... etc. Usage Usage: modis_info [OPTIONS] FILE Info tool for processed MODIS HDF5 files. Returns metadata on processed MODIS HDF5 files, both for raw and smoothed files. Options: --help Show this message and exit.","title":"Additional scripts"},{"location":"other_executables/#csv_smooth","text":"This executable allows to smooth timeseries saved in CSV format. Note For the different ways the smoother can be applied, please refer to the modis_smooth documentation. csv_smooth expects the timeseries for smoothing to be stored in the columns, therefore each column will be smoothed independently. If a column with dates or another index is present, specify it in --index-column . Header rows can be skipped using the --skip-header-rows flag. If there are column names or other metainfo (e.g. coordinates) that should not be filtered, the index of the first data cell can be specified using --data-start .","title":"csv_smooth"},{"location":"other_executables/#usage","text":"Usage: csv_smooth [OPTIONS] CSV_FILE Options: -s, --svalue FLOAT S value for smoothing (has to be log10(s)) -S, --srange FLOAT... S value range for V-curve (float log10(s) values as smin smax sstep - default 0 4 0.1) -p, --pvalue FLOAT P value for asymmetric smoothing -n, --nodata FLOAT nodata value --skip-header-rows INTEGER Number of header rows to skip --data-start INTEGER Number of row with first datapoint --index-column INTEGER Index column number --soptimize Use V-curve for s value optimization --help Show this message and exit.","title":"Usage"},{"location":"other_executables/#modis_info","text":"This executable can be used to inspect the raw/smooth HDF5 files. When supplied with the path to a file, a selection of metadata is printed to console, such as dimensions, begin/end date, last smoothing run (for smooth files), ... etc.","title":"modis_info"},{"location":"other_executables/#usage_1","text":"Usage: modis_info [OPTIONS] FILE Info tool for processed MODIS HDF5 files. Returns metadata on processed MODIS HDF5 files, both for raw and smoothed files. Options: --help Show this message and exit.","title":"Usage"},{"location":"smooth/","text":"ModisSmoothH5 Warning only implemented for MOD/MYD 11/13 products! Class representing HDF5 file containing smoothed MODIS data. A smooth HDF5 file is directly linked to it's raw HDF5 counterpart. When running the Whittaker smoother, data is read from the raw HDF5 file, smoothed and gapfilled (and if requested a temporal interpolation is performed), the resulting data is then written to the smooth HDF5 file. __init__ ( self , rawfile , targetdir , startdate = None , tempint = None ) special Initialize instance of ModisSmoothH5 class. To create an instance of ModisSmoothH5 , a full path to a ModisRawH5 HDF5 file needs to be specified. If the HDF5 file for the ModisSmoothH5 already exists in the targetdir , the file will be updated, otherwise created. To perform temporal interpolation, a desired timestep for a temporal grid has to be specified with tempint . If tempint is 5 or 10, the grid is set to a specific default. Otherwise, startdate can be used to define a default grid in conjunction with tempint . Parameters: Name Type Description Default rawfile str Full path to raw HDF5 file. required targetdir str Target directory for smooth HDF5 file. required startdate str Start date for temporal interpolation (as julian date YYYYDDD). None tempint int timesteps for temporal interpolation. None Exceptions: Type Description AssertionError If specified raw HDF5 file does not exist. create ( self ) Creates HDF5 file. If the corresponding HDF5 is not found in the target directory, it's created. Exceptions: Type Description HDF5CreationError If creation of HDF5 file fails. smooth ( self , svalue = None , p = None , soptimize = None , srange = None , nsmooth = 0 , nupdate = 0 ) Applies Whittaker smoother to the data. This method reads raw data from the raw HDF5 and applies the Whittaker filter according to the parameters supplied. The resulting filtered data is then stored in the smooth HDF5 file. The parameters relevant for derermining which Whittaker variant is supplied, are svalue , soptimize and p : If the soptimize flag is True , the V-curve optimization is performed. If in addition a p value is supplied, the assymmetric V-curve optimization is performed instead. In both cases, the range of S values to optimize in can be supplied with srange . If soptimize is False and a svalue is passed, smoothing using that fixed value (needs to be log10 of S) for each pixel is performed. If a p value is passed, asymmetric smoothing with the fixed value is performed instead. If none of svalue and soptimize is specified, a previosuly initialized S value for each pixel will be read from grid and used for the smoothing. Using nsmooth and nupdate , the number or raw timesteps used for smoothing, and the number of filtered timesteps updated in the smooth HDF5 target file, can be adjusted. Parameters: Name Type Description Default svalue float Log10 value of smoothing parameter S (for fixed smoothing). None p float P value for asymmetric smoothing. None soptimize bool Flag for V-curve optimization. None srange ndarray S-range for V-curve optimization. None nsmooth int Number of raw timesteps for smoothing. 0 nupdate int Number of smooth timesteps updated in file. 0 Exceptions: Type Description AssertionError If smooth HDF5 file does not exist ( create needs to be executed before) ValueError If nsmooth is smaller than nupdate . ValueError If srange is not specified as a numpy array with expected dimensions. HDF5WriteError If write to HDF5 file fails.","title":"modis.smooth"},{"location":"smooth/#modissmoothh5","text":"Warning only implemented for MOD/MYD 11/13 products! Class representing HDF5 file containing smoothed MODIS data. A smooth HDF5 file is directly linked to it's raw HDF5 counterpart. When running the Whittaker smoother, data is read from the raw HDF5 file, smoothed and gapfilled (and if requested a temporal interpolation is performed), the resulting data is then written to the smooth HDF5 file.","title":"ModisSmoothH5"},{"location":"smooth/#modape.modis.smooth.ModisSmoothH5.__init__","text":"Initialize instance of ModisSmoothH5 class. To create an instance of ModisSmoothH5 , a full path to a ModisRawH5 HDF5 file needs to be specified. If the HDF5 file for the ModisSmoothH5 already exists in the targetdir , the file will be updated, otherwise created. To perform temporal interpolation, a desired timestep for a temporal grid has to be specified with tempint . If tempint is 5 or 10, the grid is set to a specific default. Otherwise, startdate can be used to define a default grid in conjunction with tempint . Parameters: Name Type Description Default rawfile str Full path to raw HDF5 file. required targetdir str Target directory for smooth HDF5 file. required startdate str Start date for temporal interpolation (as julian date YYYYDDD). None tempint int timesteps for temporal interpolation. None Exceptions: Type Description AssertionError If specified raw HDF5 file does not exist.","title":"__init__()"},{"location":"smooth/#modape.modis.smooth.ModisSmoothH5.create","text":"Creates HDF5 file. If the corresponding HDF5 is not found in the target directory, it's created. Exceptions: Type Description HDF5CreationError If creation of HDF5 file fails.","title":"create()"},{"location":"smooth/#modape.modis.smooth.ModisSmoothH5.smooth","text":"Applies Whittaker smoother to the data. This method reads raw data from the raw HDF5 and applies the Whittaker filter according to the parameters supplied. The resulting filtered data is then stored in the smooth HDF5 file. The parameters relevant for derermining which Whittaker variant is supplied, are svalue , soptimize and p : If the soptimize flag is True , the V-curve optimization is performed. If in addition a p value is supplied, the assymmetric V-curve optimization is performed instead. In both cases, the range of S values to optimize in can be supplied with srange . If soptimize is False and a svalue is passed, smoothing using that fixed value (needs to be log10 of S) for each pixel is performed. If a p value is passed, asymmetric smoothing with the fixed value is performed instead. If none of svalue and soptimize is specified, a previosuly initialized S value for each pixel will be read from grid and used for the smoothing. Using nsmooth and nupdate , the number or raw timesteps used for smoothing, and the number of filtered timesteps updated in the smooth HDF5 target file, can be adjusted. Parameters: Name Type Description Default svalue float Log10 value of smoothing parameter S (for fixed smoothing). None p float P value for asymmetric smoothing. None soptimize bool Flag for V-curve optimization. None srange ndarray S-range for V-curve optimization. None nsmooth int Number of raw timesteps for smoothing. 0 nupdate int Number of smooth timesteps updated in file. 0 Exceptions: Type Description AssertionError If smooth HDF5 file does not exist ( create needs to be executed before) ValueError If nsmooth is smaller than nupdate . ValueError If srange is not specified as a numpy array with expected dimensions. HDF5WriteError If write to HDF5 file fails.","title":"smooth()"},{"location":"window/","text":"ModisMosaic Warning only implemented for MOD/MYD 11/13 products! Class for subsetting or mosaicing HDF5 files. This class can be used to mosaic multiple raw or smooth HDF5 files or to subset a single HDF5 file (both tiled and global products). The generated data will exported as GeoTIFF using the Python GDAL bindings. __init__ ( self , input_files ) special Initialize instance of ModisMosaic class. If multiple HDF5 files are specified as input_files , a mosaic of the files will be created (need to be of same product and tile/global). It's required that all of the files share the same temporal axis. If only one file is specified, either the full file is exported or a subset is created. Parameters: Name Type Description Default input_files List[str] List of paths to HDF5 files for mosaicing. required Exceptions: Type Description HDF5MosaicError If temporal axis is not conistent with multiple files (i.e. contained dates don't line up) generate_mosaics ( self , dataset , targetdir , target_srs , aoi = None , overwrite = False , force_doy = False , prefix = None , start = None , stop = None , clip_valid = False , round_int = None , ** kwargs ) Generate GeoTIFF mosaics/subsets. This method is creating a GeoTiff mosaic/subsets from the HDF5 files passed to the class instance. Internally, a virtual raster for each timestep is created, warped to the desired SRS ( target_srs ) and then optionally clipped to the area of interest ( aoi as list of corner coordinates). If the dataset is already in the desired SRS, the warping is skipped. To transform the resolution of meters to degrees in EPSG:4326 , the original resolution of the dataset in meters is divided by 112000 . The exported dates can be limited with start and stop. By default, existing GeoTIFFs are not overwritten. This can be forced with setting overwrite to True . The data to be exported can be clipped to the valid data range of the MODIS product being processed (if applicable) to the setting clip_valid to True . The data can also be rounded by passing an integer to round_int . This integer will be passed on to np.round . To modify the output naming, a prefix can be specified and if force_doy is set to True , the timestamp will be forced to the format YYYYjDDD independent of temporal interpolation. More fine scaled adjustment of the output datasets can be achieved with passing keyword arguments to gdal.WarpOptions and gdal.TranslateOptions . Parameters: Name Type Description Default dataset type Dataset to mosaic (data or sgrid). required targetdir type Target directory for output Tiffs. required target_srs str Target spatial reference (as expected by gdalwarp). required aoi List[float] AOI bouning box as ULX,ULY,LRX,LRY. None overwrite bool Flag for overwriting existing Tiffs. False force_doy bool Force DOY filenaming instead of VAM labels. False prefix str Perfix for Tiff filenames. None start date Start date for mosaics. None stop date Stop date for mosaics. None clip_valid bool Clip values to valid range for MODIS product. False round_int int Round the output. None **kwargs type **kwags passed on to gdal.WarpOptions and gdal.TranslateOptions . {} Exceptions: Type Description ValueError If dataset supplied does not exists in files. AssertionError If write fails.","title":"modis.window"},{"location":"window/#modismosaic","text":"Warning only implemented for MOD/MYD 11/13 products! Class for subsetting or mosaicing HDF5 files. This class can be used to mosaic multiple raw or smooth HDF5 files or to subset a single HDF5 file (both tiled and global products). The generated data will exported as GeoTIFF using the Python GDAL bindings.","title":"ModisMosaic"},{"location":"window/#modape.modis.window.ModisMosaic.__init__","text":"Initialize instance of ModisMosaic class. If multiple HDF5 files are specified as input_files , a mosaic of the files will be created (need to be of same product and tile/global). It's required that all of the files share the same temporal axis. If only one file is specified, either the full file is exported or a subset is created. Parameters: Name Type Description Default input_files List[str] List of paths to HDF5 files for mosaicing. required Exceptions: Type Description HDF5MosaicError If temporal axis is not conistent with multiple files (i.e. contained dates don't line up)","title":"__init__()"},{"location":"window/#modape.modis.window.ModisMosaic.generate_mosaics","text":"Generate GeoTIFF mosaics/subsets. This method is creating a GeoTiff mosaic/subsets from the HDF5 files passed to the class instance. Internally, a virtual raster for each timestep is created, warped to the desired SRS ( target_srs ) and then optionally clipped to the area of interest ( aoi as list of corner coordinates). If the dataset is already in the desired SRS, the warping is skipped. To transform the resolution of meters to degrees in EPSG:4326 , the original resolution of the dataset in meters is divided by 112000 . The exported dates can be limited with start and stop. By default, existing GeoTIFFs are not overwritten. This can be forced with setting overwrite to True . The data to be exported can be clipped to the valid data range of the MODIS product being processed (if applicable) to the setting clip_valid to True . The data can also be rounded by passing an integer to round_int . This integer will be passed on to np.round . To modify the output naming, a prefix can be specified and if force_doy is set to True , the timestamp will be forced to the format YYYYjDDD independent of temporal interpolation. More fine scaled adjustment of the output datasets can be achieved with passing keyword arguments to gdal.WarpOptions and gdal.TranslateOptions . Parameters: Name Type Description Default dataset type Dataset to mosaic (data or sgrid). required targetdir type Target directory for output Tiffs. required target_srs str Target spatial reference (as expected by gdalwarp). required aoi List[float] AOI bouning box as ULX,ULY,LRX,LRY. None overwrite bool Flag for overwriting existing Tiffs. False force_doy bool Force DOY filenaming instead of VAM labels. False prefix str Perfix for Tiff filenames. None start date Start date for mosaics. None stop date Stop date for mosaics. None clip_valid bool Clip values to valid range for MODIS product. False round_int int Round the output. None **kwargs type **kwags passed on to gdal.WarpOptions and gdal.TranslateOptions . {} Exceptions: Type Description ValueError If dataset supplied does not exists in files. AssertionError If write fails.","title":"generate_mosaics()"}]}